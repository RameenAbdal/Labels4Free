<!doctype html>
<html lang="en">

<!-- === Header Starts === -->
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>SEAN</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js"></script>
  <link href="./assets/bootstrap.min.css" rel="stylesheet">
  <link href="./assets/font.css" rel="stylesheet" type="text/css">
  <link href="./assets/style.css" rel="stylesheet" type="text/css">
</head>
<!-- === Header Ends === -->


<body>


<!-- === Home Section Starts === -->
<div class="container">
  <div class="title" style="margin: 20pt auto;font-size: 24pt;">
    Labels4Free: Unsupervised Segmentation using StyleGAN
  </div>
  <div class="author">
    <a href="https://scholar.google.com/citations?user=kEQimk0AAAAJ&hl=en" target="_blank">Rameen Abdal</a><sup>1</sup>&nbsp;
    <a href="https://scholar.google.com/citations?user=Gn8URq0AAAAJ&hl=en" target="_blank">Peihao Zhu</a><sup>1</sup>&nbsp;
    <a href="http://www0.cs.ucl.ac.uk/staff/n.mitra/" target="_blank">Niloy Mitra</a><sup>2</sup>&nbsp;
    <a href="http://peterwonka.net/" target="_blank">Peter Wonka</a><sup>1</sup>&nbsp;
  </div>
  <div class="institution">
    <sup>1</sup>KAUST <br>
    <sup>2</sup> UCL, Adobe Research
  </div>
  <div class="link">
    <a href="" target="_blank">[Paper]</a>&nbsp;
    <a href="./assets/bibtex.txt" target="_blank">[Bibtex]</a>

  </div>
  <div class="teaser">
    <img src="./assets/Teaser.png">
  </div>
  <div class="body">
  We propose an unsupervised segmentation framework that enables foreground/background separation for raw input images. At the core of our framework is an unsupervised network,  which segments class-specific StyleGAN images, and is used to generate segmentation masks for training supervised segmentation networks.
  </div>
</div>
<!-- === Home Section Ends === -->


<!--====== Overview Section Starts ======-->
<div class="container">
  <div class="title">Abstract</div>
  <div class="body">
     We propose an unsupervised segmentation framework for StyleGAN generated objects. We build on two main observations. First, the features generated by StyleGAN hold valuable information that can be utilized towards training segmentation networks. Second, the foreground and background can often be treated to be largely independent and be composited in different ways. For our solution, we propose to augment the StyleGAN2 generator architecture with a segmentation branch and to split the generator into a foreground and background network. 
    This enables us to generate soft segmentation masks for the foreground object in an unsupervised fashion. On multiple object classes, we report comparable results against state-of-the-art supervised segmentation networks, while against the best unsupervised segmentation approach we demonstrate a clear improvement, both in qualitative and quantitative metrics.
  </div>
</div>
<!--====== Overview Section Ends ======-->


<!--====== Architecture Section Starts ======-->
<div class="container">
  <div class="title">Network Architecture</div>

  <div class="teaser">
    <img src="./assets/Pipeline.png"  style="width: 100%">
    <img src="./assets/Arch2.png" style="width: 100%">
  </div>
  <div class="body">
    Our unsupervised segmentation network that makes use of pretrained generators G and G_bg to simultaneously train a segmentation Alpha network (A) and a `weak' discriminator D, without requiring supervision for ground truth masks.
  </div>
</div>
<!--====== Architecture Section Ends ======-->


<!--====== Results Section Starts ======-->
<div class="container">
  <div class="title">Results</div>
  <div class="results">1. Face Segmentation</div>
  <div class="teaser">
    <img src="./assets/Face.png" style="width: 100%">
  </div>
  <div class="body">
  Qualitative results of our unsupervised framework on StyleGAN2 trained on FFHQ compared with BiSeNet trained on CelebA-HQ Masks. Note that the green and red areas are the `False Positives' and  `False Negatives' with respect to the foreground in ground truth.  We report our results on two truncation levels.
  </div>

  <div class="results">2. LSUN-Objects Segmentation</div>
  <div class="teaser">
    <img src="./assets/LSUN.png" style="width: 100%">
  </div>
  <div class="body">
  Qualitative results of our unsupervised framework on StyleGAN2 trained on LSUN-Horse, LSUN-Cat and LSUN-Car (LSUN-Object) compared with Detectron 2 trained on MS-COCO. Note that the green and red areas are the `False Positives' and  `False Negatives' with respect to the foreground in ground truth.
  </div>


  <div class="results">3. Image Editing</div>
  <div class="teaser">
    <img src="./assets/Styleflow.png" style="width: 85%">
  </div>
  <div class="body">
  Our method achieves better background preservation compared to original semantic edits in StyleFlow. For the real image, we first obtain a background layer, segmented using Label4Free and then completed using ContentAwareFill, and then for each edit using StyleFlow, we again segment them using our method and then composite back with the completed background layer (obtained above). Please compare the first row versus third rows.
  </div>
</div>
<!--====== Results Section Ends ======-->


<!--====== Bibtex Section Starts ======-->
<div class="container">
  <div class="bibtex">Bibtex</div>
<pre>

</pre>
</div>

<!--====== Bibtex Section Ends ======-->


<!--====== Bibtex Section Starts ======-->
<div class="container">
  <div class="acknowledgement">Acknowledgement</div>
  <div class="body" style="font-size: 11pt;;">
    This work was supported by the KAUST Office of Sponsored Research (OSR) under Award No. OSR-CRG2017-3426.
  </div>

</div>

<!--====== Bibtex Section Ends ======-->




<!--====== References Section Starts ======-->

<div class="container">
  <div class="ref">Related Work</div>
 
  <div class="citation">
    <img src="./assets/styleGAN.png">
    <a href="https://https://github.com/NVlabs/stylegan2">
      Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila.
      Analyzing and Improving the Image Quality of StyleGAN
      CVPR, 2020.
    </a>
  </div>
 

<!--====== References Section Ends ======-->


</body>
</html>
