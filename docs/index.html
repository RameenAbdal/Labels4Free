<!doctype html>
<html lang="en">

<!-- === Header Starts === -->
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>SEAN</title>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js"></script>
  <link href="./assets/bootstrap.min.css" rel="stylesheet">
  <link href="./assets/font.css" rel="stylesheet" type="text/css">
  <link href="./assets/style.css" rel="stylesheet" type="text/css">
</head>
<!-- === Header Ends === -->


<body>


<!-- === Home Section Starts === -->
<div class="container">
  <div class="title" style="margin: 20pt auto;font-size: 24pt;">
    Labels4Free: Unsupervised Segmentation using StyleGAN
  </div>
  <div class="author">
    <a href="https://scholar.google.com/citations?user=kEQimk0AAAAJ&hl=en" target="_blank">Rameen Abdal</a><sup>1</sup>&nbsp;
    <a href="https://scholar.google.com/citations?user=Gn8URq0AAAAJ&hl=en" target="_blank">Peihao Zhu</a><sup>1</sup>&nbsp;
    <a href="http://www0.cs.ucl.ac.uk/staff/n.mitra/" target="_blank">Niloy Mitra</a><sup>2</sup>&nbsp;
    <a href="http://peterwonka.net/" target="_blank">Peter Wonka</a><sup>1</sup>&nbsp;
  </div>
  <div class="institution">
    <sup>1</sup>KAUST <br>
    <sup>2</sup> UCL, Adobe Research
  </div>
  <div class="link">
    <a href="" target="_blank">[Paper]</a>&nbsp;
    <a href="./assets/bibtex.txt" target="_blank">[Bibtex]</a>

  </div>
  <div class="teaser">
    <img src="./assets/Teaser.png">
  </div>
  <div class="body">
  We propose an unsupervised segmentation framework that enables foreground/background separation for raw input images. At the core of our framework is an unsupervised network,  which segments class-specific StyleGAN images, and is used to generate segmentation masks for training supervised segmentation networks.
  </div>
</div>
<!-- === Home Section Ends === -->


<!--====== Overview Section Starts ======-->
<div class="container">
  <div class="title">Abstract</div>
  <div class="body">
     We propose an unsupervised segmentation framework for StyleGAN generated objects. We build on two main observations. First, the features generated by StyleGAN hold valuable information that can be utilized towards training segmentation networks. Second, the foreground and background can often be treated to be largely independent and be composited in different ways. For our solution, we propose to augment the StyleGAN2 generator architecture with a segmentation branch and to split the generator into a foreground and background network. 
    This enables us to generate soft segmentation masks for the foreground object in an unsupervised fashion. On multiple object classes, we report comparable results against state-of-the-art supervised segmentation networks, while against the best unsupervised segmentation approach we demonstrate a clear improvement, both in qualitative and quantitative metrics.
  </div>
</div>
<!--====== Overview Section Ends ======-->


<!--====== Architecture Section Starts ======-->
<div class="container">
  <div class="title">Network Architecture</div>

  <div class="teaser">
    <img src="./assets/Pipeline.png"  style="width: 100%">
    <img src="./assets/Arch2.png" style="width: 100%">
  </div>
  <div class="body">
    Our unsupervised segmentation network that makes use of pretrained generators G and G_bg to simultaneously train a segmentation network A and a `weak' discriminator D, without requiring supervision for ground truth masks.
  </div>
</div>
<!--====== Architecture Section Ends ======-->


<!--====== Results Section Starts ======-->
<div class="container">
  <div class="title">Results and Applications</div>
  <div class="results">1. Image Reconstruction</div>
  <div class="teaser">
    <img src="./assets/Rec.png" style="width: 85%">
  </div>
  <div class="body">
  Visual  comparison  of  semantic  image  synthesis  results  on  the  CelebAMask-HQ, ADE20K, CityScapes and
  Facades dataset. We compare Pix2PixHD, SPADE, and our method.
  </div>

  <div class="results">2. Image Editing</div>
  <div class="teaser">
    <img src="./assets/Image_editing.png" style="width: 85%">
  </div>
  <div class="body">
  Editing sequence on the ADE20K dataset.  (a) source image, (b) reconstruction of the source image,
  (c-f) variousedits using style images shown in the top row. The regions affected by the edits are shown as small insets.
  </div>


  <div class="results">3. Style Transfer</div>
  <div class="teaser">
    <img src="./assets/Style_transfer.png" style="width: 85%">
  </div>
  <div class="body">
  Style transfer on CelebAMask-HQ dataset.
  </div>

  <div class="results">4. Style interpolation & Style Crossover</div>
  <div class="teaser">
    <img src="./assets/Interpolation.jpg" style="width: 85%">
  </div>
  <div class="body">
  Style interpolation.  We take a mask from a source image and reconstruct with two different style images (Style1and Style2)
   that are very different from the source image. We then show interpolated results of the per-region style codes.
  </div>
  <div class="teaser">
    <img src="./assets/crossover.jpg" style="width: 85%">
  </div>
  <div class="body">
  Style crossover.  In addition to style interpolation (bottom row), we can perform crossover by selecting differentstyles per ResBlk. We show two transitions in the top two rows.
   The blue / orange bars on top of the images indicate whichstyles are used by the six ResBlks.
   We can observe that earlier layers are responsible for larger features and later layersmainly determine the color scheme.
  </div>




</div>
<!--====== Results Section Ends ======-->


<!--====== Bibtex Section Starts ======-->
<div class="container">
  <div class="bibtex">Bibtex</div>
<pre>
  @misc{zhu2019sean,
    title={SEAN: Image Synthesis with Semantic Region-Adaptive Normalization},
    author={Peihao Zhu and Rameen Abdal and Yipeng Qin and Peter Wonka},
    year={2019},
    eprint={1911.12861},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}
</pre>
</div>

<!--====== Bibtex Section Ends ======-->


<!--====== Bibtex Section Starts ======-->
<div class="container">
  <div class="acknowledgement">Acknowledgement</div>
  <div class="body" style="font-size: 11pt;;">
    We thank Wamiq Reyaz Para for helpful comments. This  work  was  supported  by  the KAUST Office of Sponsored Research (OSR) under AwardNo. OSR-CRG2018-3730.
  </div>

</div>

<!--====== Bibtex Section Ends ======-->




<!--====== References Section Starts ======-->

<div class="container">
  <div class="ref">Related Work</div>
  <div class="citation">
    <img src="./assets/SPADE.png">
    <a href="https://nvlabs.github.io/SPADE/">
      Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-YanZhu.
      Semantic image synthesis with spatially-adaptive nor-malization.
      CVPR, 2019.
    </a>
  </div>
  <div class="citation">
    <img src="./assets/styleGAN.png">
    <a href="https://github.com/NVlabs/stylegan">
      Tero Karras, Samuli Laine, Timo Aila.
      A Style-Based Generator Architecture for Generative Adversarial Networks
      CVPR, 2019.
    </a>
  </div>
  <div class="citation">
    <img src="./assets/Pix2PixHD.png">
    <a href="https://tcwang0509.github.io/pix2pixHD/">
      Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro.
      High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs.
      CVPR, 2018.
    </a>
  </div>


<!--====== References Section Ends ======-->


</body>
</html>
